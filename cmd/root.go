package cmd

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"
	"sync"
	"time"

	clibase "github.com/shouni/go-cli-base"
	"github.com/shouni/go-utils/iohandler"
	"github.com/spf13/cobra"
	"golang.org/x/sync/semaphore"

	// å¿…è¦ãªä¾å­˜é–¢ä¿‚
	"act-feed-clean-go/pkg/feed"
	"github.com/shouni/go-http-kit/pkg/httpkit"
	//	"github.com/shouni/go-utils/iohandler"
	"github.com/shouni/go-web-exact/v2/pkg/extract"
)

// (å‰ç•¥: RunFlags, Flags, ExtractedArticle ã®å®šç¾©ã¯ãã®ã¾ã¾)

// RunFlags ã¯ 'run' ã‚³ãƒãƒ³ãƒ‰å›ºæœ‰ã®ãƒ•ãƒ©ã‚°ã‚’ä¿æŒã™ã‚‹æ§‹é€ ä½“ã§ã™ã€‚
type RunFlags struct {
	LLMAPIKey     string
	FeedURL       string
	Parallel      int
	ScrapeTimeout time.Duration
}

var Flags RunFlags

// ExtractedArticle ã¯ä¸¦åˆ—å‡¦ç†ã®çµæœã‚’ä¿æŒã™ã‚‹ãŸã‚ã®æ§‹é€ ä½“ (runFeedExtractionå†…ã§ä½¿ç”¨)
type ExtractedArticle struct {
	URL     string
	Content string
	Error   error
}

// runFeedExtraction ã®æœ¬ä½“ï¼ˆä¸¦åˆ—æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã‚’ cmd ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ã«å®šç¾©ã—ã¾ã™ã€‚
func runFeedExtraction(feedURL string, parallel int, scrapeTimeout time.Duration) error {
	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second) // å…¨ä½“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
	defer cancel()

	// 1. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®åˆæœŸåŒ–
	const maxRetries = 3

	// timeoutã‚’ç¬¬1å¼•æ•°ã«æ¸¡ã—ã€WithMaxRetriesã§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¨­å®šã—ã¾ã™
	clientOptions := []httpkit.ClientOption{
		httpkit.WithMaxRetries(maxRetries),
	}
	httpClient := httpkit.New(scrapeTimeout, clientOptions...)
	extractor, nil := extract.NewExtractor(httpClient)

	// 2. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã¨URLãƒªã‚¹ãƒˆç”Ÿæˆ
	rssFeed, err := feed.FetchAndParse(ctx, httpClient, feedURL)
	if err != nil {
		return fmt.Errorf("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	urlsToProcess := feed.ExtractLinks(rssFeed)
	if len(urlsToProcess) == 0 {
		return fmt.Errorf("ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ‰åŠ¹ãªè¨˜äº‹URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
	}

	fmt.Fprintf(os.Stderr, "ğŸŒ è¨˜äº‹URL %dä»¶ã‚’æœ€å¤§ä¸¦åˆ—æ•° %d ã§æœ¬æ–‡æŠ½å‡ºä¸­...\n", len(urlsToProcess), parallel)

	// 3. ä¸¦åˆ—æŠ½å‡ºã®å®Ÿè¡Œ (ã‚»ãƒãƒ•ã‚©åˆ¶å¾¡)
	sem := semaphore.NewWeighted(int64(parallel))
	var wg sync.WaitGroup
	results := make(chan ExtractedArticle, len(urlsToProcess))

	// (ä¸¦åˆ—å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã¯å‰å›ã®å®Ÿè£…ã¨ã»ã¼åŒã˜)
	for _, url := range urlsToProcess {
		wg.Add(1)
		go func(url string) {
			defer wg.Done()
			if err := sem.Acquire(ctx, 1); err != nil {
				results <- ExtractedArticle{URL: url, Error: fmt.Errorf("ã‚»ãƒãƒ•ã‚©å–å¾—å¤±æ•—: %w", err)}
				return
			}
			defer sem.Release(1)
			content, hasBodyFound, err := extractor.FetchAndExtractText(url, ctx)
			var extractErr error
			if err != nil {
				extractErr = fmt.Errorf("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
			} else if content == "" || !hasBodyFound {
				extractErr = fmt.Errorf("URL %s ã‹ã‚‰æœ‰åŠ¹ãªæœ¬æ–‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ", url)
			}
			log.Print(extractErr)

			results <- ExtractedArticle{URL: url, Content: content, Error: err}
		}(url)
	}
	wg.Wait()
	close(results)

	// 4. çµæœã®çµåˆã¨å‡ºåŠ›æº–å‚™
	var combinedTextBuilder strings.Builder
	combinedTextBuilder.WriteString(fmt.Sprintf("# %s\n\n", rssFeed.Title))
	successCount := 0

	for res := range results {
		if res.Error != nil {
			fmt.Fprintf(os.Stderr, "âŒ æŠ½å‡ºå¤±æ•— [%s]: %v\n", res.URL, res.Error)
			continue
		}
		combinedTextBuilder.WriteString(fmt.Sprintf("## ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘ %s\n\n", res.URL))
		combinedTextBuilder.WriteString(res.Content)
		combinedTextBuilder.WriteString("\n\n---\n\n")
		successCount++
	}
	fmt.Fprintf(os.Stderr, "âœ… æŠ½å‡ºå®Œäº†ã€‚æˆåŠŸä»¶æ•°: %d / å‡¦ç†ä»¶æ•°: %d\n", successCount, len(urlsToProcess))

	// 5. çµåˆãƒ†ã‚­ã‚¹ãƒˆã®å‡ºåŠ›
	if combinedTextBuilder.Len() > 0 {
		return iohandler.WriteOutput("", []byte(combinedTextBuilder.String()))
	}

	return nil
}

// runCmdFunc ã¯ 'run' ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ãŒå‘¼ã³å‡ºã•ã‚ŒãŸã¨ãã«å®Ÿè¡Œã•ã‚Œã‚‹é–¢æ•°ã§ã™ã€‚
func runCmdFunc(cmd *cobra.Command, args []string) error {
	// APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡ç•¥åŒ–ï¼‰
	if Flags.LLMAPIKey == "" {
		Flags.LLMAPIKey = os.Getenv("GEMINI_API_KEY")
		if Flags.LLMAPIKey == "" {
			return fmt.Errorf("ã‚¨ãƒ©ãƒ¼: LLM APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚-kãƒ•ãƒ©ã‚°ã¾ãŸã¯GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
		}
	}

	// è§£æ±ºç­–: å®šç¾©ã—ãŸ runFeedExtraction ã‚’å‘¼ã³å‡ºã™
	return runFeedExtraction(Flags.FeedURL, Flags.Parallel, Flags.ScrapeTimeout)
}

// (å¾Œç•¥: addRunFlags, runCmd, Execute ã®å®šç¾©ã¯ãã®ã¾ã¾)

// addRunFlags ã¯ 'run' ã‚³ãƒãƒ³ãƒ‰ã«å›ºæœ‰ã®ãƒ•ãƒ©ã‚°ã‚’è¨­å®šã—ã¾ã™ã€‚
func addRunFlags(runCmd *cobra.Command) {
	runCmd.Flags().StringVarP(&Flags.LLMAPIKey, "llm-api-key", "k", "", "Gemini APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ãŒå„ªå…ˆ)")
	runCmd.Flags().StringVarP(&Flags.FeedURL, "feed-url", "f", "https://news.yahoo.co.jp/rss/categories/it.xml", "å‡¦ç†å¯¾è±¡ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰URL")
	runCmd.Flags().IntVarP(&Flags.Parallel, "parallel", "p", 10, "Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®æœ€å¤§åŒæ™‚ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°")
	runCmd.Flags().DurationVarP(&Flags.ScrapeTimeout, "scraper-timeout", "s", 15*time.Second, "Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®HTTPã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“")
}

var runCmd = &cobra.Command{
	Use:   "run",
	Short: "RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã€ä¸¦åˆ—æŠ½å‡ºã€AIã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚",
	Long:  "RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰URLã‚’æŠ½å‡ºã—ã€è¨˜äº‹æœ¬æ–‡ã‚’ä¸¦åˆ—ã§å–å¾—å¾Œã€AIã§æ§‹é€ åŒ–ãƒ»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚",
	RunE:  runCmdFunc,
}

// Execute ã¯ã€CLIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚
func Execute() {
	// runCmd ã«ãƒ•ãƒ©ã‚°ã‚’è¿½åŠ 
	addRunFlags(runCmd)

	clibase.Execute(
		"act-feed-clean-go",
		nil,
		nil,
		runCmd,
	)
}
