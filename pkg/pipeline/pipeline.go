package pipeline

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	"act-feed-clean-go/pkg/cleaner"
	"act-feed-clean-go/pkg/feed"
	"act-feed-clean-go/pkg/scraper"
	"act-feed-clean-go/pkg/types"

	"github.com/shouni/go-http-kit/pkg/httpkit"
	"github.com/shouni/go-utils/iohandler"
	"github.com/shouni/go-web-exact/v2/pkg/extract"
)

// Pipeline ã¯è¨˜äº‹ã®å–å¾—ã‹ã‚‰çµåˆã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ç®¡ç†ã—ã¾ã™ã€‚
type Pipeline struct {
	Client    *httpkit.Client
	Extractor *extract.Extractor

	Scraper scraper.Scraper
	Cleaner *cleaner.Cleaner

	// è¨­å®šå€¤
	Parallel  int
	Verbose   bool
	LLMAPIKey string // LLMå‡¦ç†ã®ãŸã‚ã«APIã‚­ãƒ¼ã‚’ä¿æŒ
}

// New ã¯æ–°ã—ã„ Pipeline ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã—ã€ä¾å­˜é–¢ä¿‚ã‚’æ³¨å…¥ã—ã¾ã™ã€‚
// LLMAPIKeyã¯cmd/root.goã‹ã‚‰æ¸¡ã•ã‚Œã¾ã™ã€‚
func New(client *httpkit.Client, parallel int, verbose bool, llmAPIKey string) (*Pipeline, error) {

	// 1. Extractorã®åˆæœŸåŒ– (ScraperãŒä¾å­˜)
	extractor, err := extract.NewExtractor(client)
	if err != nil {
		return nil, fmt.Errorf("ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	// 2. Scraperã®åˆæœŸåŒ– (ä¸¦åˆ—å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–)
	parallelScraper := scraper.NewParallelScraper(extractor, parallel)

	// 3. Cleanerã®åˆæœŸåŒ– (AIå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–)
	// NewCleanerã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«åã¨verboseãƒ•ãƒ©ã‚°ã‚’æ¸¡ã™
	const defaultMapModel = cleaner.DefaultModelName
	const defaultReduceModel = cleaner.DefaultModelName
	llmCleaner, err := cleaner.NewCleaner(defaultMapModel, defaultReduceModel, verbose)
	if err != nil {
		return nil, fmt.Errorf("ã‚¯ãƒªãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	return &Pipeline{
		Client:    client,
		Extractor: extractor,
		Scraper:   parallelScraper, // æ³¨å…¥
		Cleaner:   llmCleaner,      // æ³¨å…¥
		Parallel:  parallel,
		Verbose:   verbose,
		LLMAPIKey: llmAPIKey, // ä¿æŒ
	}, nil
}

// Run ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã€è¨˜äº‹ã®ä¸¦åˆ—æŠ½å‡ºã€AIå‡¦ç†ã€ãŠã‚ˆã³I/Oå‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
func (p *Pipeline) Run(ctx context.Context, feedURL string) error {

	// --- 1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã¨URLãƒªã‚¹ãƒˆç”Ÿæˆ ---
	rssFeed, err := feed.FetchAndParse(ctx, p.Client, feedURL)
	if err != nil {
		return fmt.Errorf("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	urlsToScrape := make([]string, 0, len(rssFeed.Items))
	// Titleã¯Extractorã§ã¯ãªãRSSã‹ã‚‰å–å¾—ã™ã‚‹ãŸã‚ã€ä¸€æ™‚çš„ãªãƒãƒƒãƒ—ã§ä¿æŒ
	articleTitlesMap := make(map[string]string)

	for _, item := range rssFeed.Items {
		if item.Link != "" && item.Title != "" {
			urlsToScrape = append(urlsToScrape, item.Link)
			articleTitlesMap[item.Link] = item.Title
		}
	}

	if len(urlsToScrape) == 0 {
		return fmt.Errorf("ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ‰åŠ¹ãªè¨˜äº‹URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
	}

	fmt.Fprintf(os.Stderr, "ğŸŒ è¨˜äº‹URL %dä»¶ã‚’æœ€å¤§ä¸¦åˆ—æ•° %d ã§æœ¬æ–‡æŠ½å‡ºä¸­...\n", len(urlsToScrape), p.Parallel)

	// --- 2. Scraperã«ã‚ˆã‚‹ä¸¦åˆ—æŠ½å‡ºã®å®Ÿè¡Œ ---
	// Scraperã«å‡¦ç†ã‚’å§”è­²ã€‚
	results := p.Scraper.ScrapeInParallel(ctx, urlsToScrape)

	// --- 3. æŠ½å‡ºçµæœã®ç¢ºèªã¨AIå‡¦ç†ã®åˆ†å² ---
	successCount := 0
	for _, res := range results {
		if res.Error == nil {
			successCount++
		} else if p.Verbose {
			// æŠ½å‡ºã‚¨ãƒ©ãƒ¼ã‚’Verboseãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å‡ºåŠ›
			log.Printf("âŒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼ [%s]: %v", res.URL, res.Error)
		}
	}

	fmt.Fprintf(os.Stderr, "âœ… æŠ½å‡ºå®Œäº†ã€‚æˆåŠŸä»¶æ•°: %d / å‡¦ç†ä»¶æ•°: %d\n", successCount, len(urlsToScrape))

	if successCount == 0 {
		return fmt.Errorf("å‡¦ç†ã™ã¹ãè¨˜äº‹æœ¬æ–‡ãŒä¸€ã¤ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
	}

	// AIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹ã‚’LLMAPIKeyã®æœ‰ç„¡ã§åˆ¤æ–­
	if p.LLMAPIKey == "" {
		return p.processWithoutAI(rssFeed.Title, results, articleTitlesMap)
	}

	// --- 4. AIå‡¦ç†ã®å®Ÿè¡Œ (Cleanerã«ã‚ˆã‚‹ Map-Reduce) ---
	fmt.Fprintln(os.Stderr, "\nğŸ¤– LLMå‡¦ç†é–‹å§‹ (Cleanerã«ã‚ˆã‚‹ Map-Reduce)...")

	// 4-1. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµåˆ (æˆåŠŸã—ãŸçµæœã®ã¿ã‚’çµåˆ)
	combinedTextForAI := cleaner.CombineContents(results)

	// 4-2. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨æ§‹é€ åŒ–ã®å®Ÿè¡Œ
	// LLMAPIKeyã‚’Overrideã¨ã—ã¦Cleanerã«æ¸¡ã™
	structuredText, err := p.Cleaner.CleanAndStructureText(ctx, combinedTextForAI, p.LLMAPIKey)
	if err != nil {
		// Cleanerã‹ã‚‰è¿”ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ã‚’ãƒ©ãƒƒãƒ—ã—ã¦è¿”ã™
		return fmt.Errorf("AIã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	// --- 5. AIå‡¦ç†çµæœã®å‡ºåŠ› ---
	fmt.Fprintln(os.Stderr, "\n--- ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå®Œäº† (AIæ§‹é€ åŒ–æ¸ˆã¿) ---")
	// iohandler ã¯ stringã§ã¯ãªã []byteã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«ä¿®æ­£ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹
	return iohandler.WriteOutput("", []byte(structuredText))
}

// processWithoutAI ã¯ LLMAPIKeyãŒãªã„å ´åˆã«å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†
func (p *Pipeline) processWithoutAI(feedTitle string, results []types.URLResult, titlesMap map[string]string) error {
	var combinedTextBuilder strings.Builder
	combinedTextBuilder.WriteString(fmt.Sprintf("# %s\n\n", feedTitle))

	for _, res := range results {
		if res.Error != nil {
			// AIå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰ã§ã‚‚å¤±æ•—ã—ãŸURLã‚’é€šçŸ¥
			fmt.Fprintf(os.Stderr, "âŒ æŠ½å‡ºå¤±æ•— [%s]: %v\n", res.URL, res.Error)
			continue
		}

		articleTitle := titlesMap[res.URL]
		if articleTitle == "" {
			articleTitle = res.URL
		}

		// è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’çµåˆ
		combinedTextBuilder.WriteString(fmt.Sprintf("## ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘ %s\n\n", articleTitle))
		combinedTextBuilder.WriteString(res.Content)
		combinedTextBuilder.WriteString("\n\n---\n\n")
	}

	combinedText := combinedTextBuilder.String()

	fmt.Fprintln(os.Stderr, "\n--- ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆçµæœ (AIå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—) ---")

	// iohandler ã‚’ä½¿ç”¨ã—ã¦ []byte ã§å‡ºåŠ›
	return iohandler.WriteOutput("", []byte(combinedText))
}
