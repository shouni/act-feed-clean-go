package pipeline

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	"act-feed-clean-go/pkg/cleaner"
	"act-feed-clean-go/pkg/feed"
	"act-feed-clean-go/pkg/scraper"
	"act-feed-clean-go/pkg/types"

	"github.com/shouni/go-http-kit/pkg/httpkit"
	"github.com/shouni/go-utils/iohandler"
	"github.com/shouni/go-web-exact/v2/pkg/extract"
)

// Pipeline ã¯è¨˜äº‹ã®å–å¾—ã‹ã‚‰çµåˆã¾ã§ã®ä¸€é€£ã®æµã‚Œã‚’ç®¡ç†ã—ã¾ã™ã€‚
type Pipeline struct {
	Client    *httpkit.Client
	Extractor *extract.Extractor

	Scraper scraper.Scraper
	Cleaner *cleaner.Cleaner

	// è¨­å®šå€¤
	Parallel  int
	Verbose   bool
	LLMAPIKey string // ä¿®æ­£: LLMå‡¦ç†ã®ãŸã‚ã«APIã‚­ãƒ¼ã‚’ä¿æŒ
}

// New ã¯æ–°ã—ã„ Pipeline ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã—ã€ä¾å­˜é–¢ä¿‚ã‚’æ³¨å…¥ã—ã¾ã™ã€‚
// LLMAPIKeyã¯cmd/root.goã‹ã‚‰æ¸¡ã•ã‚Œã¾ã™ã€‚
func New(client *httpkit.Client, parallel int, verbose bool, llmAPIKey string) (*Pipeline, error) { // ä¿®æ­£: llmAPIKeyå¼•æ•°ã‚’è¿½åŠ 

	// 1. Extractorã®åˆæœŸåŒ– (ScraperãŒä¾å­˜)
	extractor, err := extract.NewExtractor(client)
	if err != nil {
		return nil, fmt.Errorf("ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	// 2. Scraperã®åˆæœŸåŒ– (ä¸¦åˆ—å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–)
	parallelScraper := scraper.NewParallelScraper(extractor, parallel)

	// 3. Cleanerã®åˆæœŸåŒ– (AIå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–)
	// ä¿®æ­£: NewCleanerã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«åã¨verboseãƒ•ãƒ©ã‚°ã‚’æ¸¡ã™
	// NOTE: ãƒ¢ãƒ‡ãƒ«åã‚’CLIã‹ã‚‰è¨­å®šã™ã‚‹ãƒ•ãƒ©ã‚°ãŒãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåã‚’ä½¿ç”¨
	const defaultMapModel = cleaner.DefaultModelName
	const defaultReduceModel = cleaner.DefaultModelName
	llmCleaner, err := cleaner.NewCleaner(defaultMapModel, defaultReduceModel, verbose)
	if err != nil {
		return nil, fmt.Errorf("ã‚¯ãƒªãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	return &Pipeline{
		Client:    client,
		Extractor: extractor,
		Scraper:   parallelScraper, // æ³¨å…¥
		Cleaner:   llmCleaner,      // æ³¨å…¥
		Parallel:  parallel,
		Verbose:   verbose,
		LLMAPIKey: llmAPIKey, // ä¿æŒ
	}, nil
}

// Run ã¯ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã€è¨˜äº‹ã®ä¸¦åˆ—æŠ½å‡ºã€AIå‡¦ç†ã€ãŠã‚ˆã³I/Oå‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
func (p *Pipeline) Run(ctx context.Context, feedURL string) error {

	// --- 1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã¨URLãƒªã‚¹ãƒˆç”Ÿæˆ ---
	rssFeed, err := feed.FetchAndParse(ctx, p.Client, feedURL)
	if err != nil {
		return fmt.Errorf("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	urlsToScrape := make([]string, 0, len(rssFeed.Items))
	// Titleã¯Extractorã§ã¯ãªãRSSã‹ã‚‰å–å¾—ã™ã‚‹ãŸã‚ã€ä¸€æ™‚çš„ãªãƒãƒƒãƒ—ã§ä¿æŒ (types.URLResultãŒTitleã‚’æŒãŸãªã„ãŸã‚)
	articleTitlesMap := make(map[string]string)

	for _, item := range rssFeed.Items {
		if item.Link != "" && item.Title != "" {
			urlsToScrape = append(urlsToScrape, item.Link)
			articleTitlesMap[item.Link] = item.Title
		}
	}

	if len(urlsToScrape) == 0 {
		return fmt.Errorf("ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ‰åŠ¹ãªè¨˜äº‹URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
	}

	fmt.Fprintf(os.Stderr, "ğŸŒ è¨˜äº‹URL %dä»¶ã‚’æœ€å¤§ä¸¦åˆ—æ•° %d ã§æœ¬æ–‡æŠ½å‡ºä¸­...\n", len(urlsToScrape), p.Parallel)

	// --- 2. Scraperã«ã‚ˆã‚‹ä¸¦åˆ—æŠ½å‡ºã®å®Ÿè¡Œ ---
	// Scraperã«å‡¦ç†ã‚’å§”è­²ã€‚ã‚»ãƒãƒ•ã‚©åˆ¶å¾¡ã‚„Goroutineç®¡ç†ã¯ã™ã¹ã¦ scraper.ScrapeInParallel ãŒæ‹…å½“ã€‚
	results := p.Scraper.ScrapeInParallel(ctx, urlsToScrape)

	// --- 3. æŠ½å‡ºçµæœã®ç¢ºèªã¨AIå‡¦ç†ã®åˆ†å² ---
	successCount := 0
	for _, res := range results {
		if res.Error == nil {
			successCount++
		} else if p.Verbose {
			log.Printf("âŒ æŠ½å‡ºã‚¨ãƒ©ãƒ¼ [%s]: %v", res.URL, res.Error)
		}
	}

	fmt.Fprintf(os.Stderr, "âœ… æŠ½å‡ºå®Œäº†ã€‚æˆåŠŸä»¶æ•°: %d / å‡¦ç†ä»¶æ•°: %d\n", successCount, len(urlsToScrape))

	if successCount == 0 {
		return fmt.Errorf("å‡¦ç†ã™ã¹ãè¨˜äº‹æœ¬æ–‡ãŒä¸€ã¤ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
	}

	// AIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹ã‚’LLMAPIKeyã®æœ‰ç„¡ã§åˆ¤æ–­
	if p.LLMAPIKey == "" {
		return p.processWithoutAI(rssFeed.Title, results, articleTitlesMap)
	}

	// --- 4. AIå‡¦ç†ã®å®Ÿè¡Œ (Cleanerã«ã‚ˆã‚‹ Map-Reduce) ---
	fmt.Fprintln(os.Stderr, "\nğŸ¤– LLMå‡¦ç†é–‹å§‹ (Cleanerã«ã‚ˆã‚‹ Map-Reduce)...")

	// 4-1. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®çµåˆ (Cleanerã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’ä½¿ç”¨)
	combinedTextForAI := cleaner.CombineContents(results)

	// 4-2. ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨æ§‹é€ åŒ–ã®å®Ÿè¡Œ
	structuredText, err := p.Cleaner.CleanAndStructureText(ctx, combinedTextForAI, p.LLMAPIKey)
	if err != nil {
		return fmt.Errorf("AIã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: %w", err)
	}

	// --- 5. AIå‡¦ç†çµæœã®å‡ºåŠ› ---
	fmt.Fprintln(os.Stderr, "\n--- ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆå®Œäº† (AIæ§‹é€ åŒ–æ¸ˆã¿) ---")
	return iohandler.WriteOutput("", []byte(structuredText))
}

// processWithoutAI ã¯ LLMAPIKeyãŒãªã„å ´åˆã«å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†
func (p *Pipeline) processWithoutAI(feedTitle string, results []types.URLResult, titlesMap map[string]string) error {
	var combinedTextBuilder strings.Builder
	combinedTextBuilder.WriteString(fmt.Sprintf("# %s\n\n", feedTitle))

	for _, res := range results {
		if res.Error != nil {
			fmt.Fprintf(os.Stderr, "âŒ æŠ½å‡ºå¤±æ•— [%s]: %v\n", res.URL, res.Error)
			continue
		}

		articleTitle := titlesMap[res.URL]
		if articleTitle == "" {
			articleTitle = res.URL
		}

		// è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’çµåˆ
		combinedTextBuilder.WriteString(fmt.Sprintf("## ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã€‘ %s\n\n", articleTitle))
		combinedTextBuilder.WriteString(res.Content)
		combinedTextBuilder.WriteString("\n\n---\n\n")
	}

	combinedText := combinedTextBuilder.String()

	fmt.Fprintln(os.Stderr, "\n--- ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆçµæœ (AIå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—) ---")

	// iohandler ã‚’ä½¿ç”¨ã—ã¦ []byte ã§å‡ºåŠ›
	return iohandler.WriteOutput("", []byte(combinedText))
}
